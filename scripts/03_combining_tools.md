# ğŸ§© Combining Tools: Power Moves in the Unix Shell

As a data analyst, my daily workflows often involve transforming, cleaning, and summarizing large datasetsâ€”fast. The real power of the Unix shell isnâ€™t in individual commands, but in how you can chain simple tools together to solve complex, real-world problems. This chapter is packed with my go-to pipelines for efficient data wrangling and automation.

## âœ¨ What Youâ€™ll Learn
  * **Piping and Redirection**: Seamlessly send output from one tool to the next for streamlined workflows.
  * **Selecting Data**: Combine ```cut```, ```grep```, ```head```, and more to get exactly the rows or columns you need.
  * **Sorting Results**: Use ```sort``` to order your dataâ€”by dates, numbers, or categories.
  * **Removing Duplicates**: Apply ```uniq``` after ```sort``` to ensure clean, non-redundant datasets.
  * **Real-World Automation**: Chain tools to automate repetitive data-cleaning or QA steps in analytics projects.

---

## ğŸ› ï¸ Real-World Scenarios
  * Extracting top customer records from a CSV and emailing a summaryâ€”all in one line.
  * Sorting log files by timestamp and identifying unique errors in seconds.
  * Filtering, deduplicating, and summarizing millions of rows in seconds, before loading into Python or SQL.

---

# ğŸš© Storing Command Output: DataOps Best Practice
In real-world analytics projects, you often want to save results from the shell for further analysis, sharing, or automation. This is where output redirection (```>```) is essential.

#### *Example:*

  ```tail -n 5 seasonal/winter.csv > last.csv```

  * ```tail -n 5``` gets the last 5 lines from your data.
  * ```>``` redirects the output into ```last.csv``` (instead of showing it in the terminal).
  * Ready for reporting, review, or as input to the next step in your pipeline.


### ğŸ’¡ *Why this matters:*
  * **Automate data** hand-off between tools and teammates
  * Save **intermediate results** for debugging or documentation
  * **Build** repeatable, versioned **pipelines**
  
---

# ğŸ” Chaining Commands: Input-Output Magic
You rarely need just the first or last rows of a file. What if you want lines 3â€“5 from a huge CSV? Chaining commands is how I build flexible, repeatable pipelines:

### *Workflow Example:*
Extract lines 3â€“5 from a file and save for review.

  ```head -n 5 seasonal/winter.csv | tail -n 3 > midrows.csv```

  * ```head -n 5``` gets the first 5 lines
  * ```tail -n 3``` selects the last 3 of those (i.e., lines 3â€“5)
  * ```>``` writes result to midrows.csv

<br>

ğŸ’¡ *This patternâ€”combining ```head```, ```tail```, and redirection â€” is widely used in real data operations, especially for log analysis, automated quality checks, and pipeline validation.*

---

# ğŸ§© Combining Commands with Pipes
Instead of cluttering your workspace with intermediate files, use pipes (```|```) to pass data between commands seamlessly.

### *Example:*
Extract all unique tooth types (except the header) from summer data:

```cut -d , -f 2 seasonal/summer.csv | grep -v Tooth | sort | uniq```

 * ```cut``` grabs column 2
 * ```grep -v Tooth``` removes header
 * ```sort | uniq``` deduplicates

One-liners like this mean less manual cleanup, more reproducible analytics, and much faster data exploration.


### ğŸ’¡ *Business value:*
 * **Cleaner workflow:** Avoids creating temporary files like ```step1.csv```, ```step2.csv```, etc.
 * **Readability:** Keeps the entire transformation in a single, readable line.
 * **Performance:** Reduces disk I/O and clutter.
   
---


# ğŸ”¢ Counting Records Fast with wc
When dealing with batch files or big datasets, I use ```wc``` (word count) for a quick record tally:

```wc -l seasonal/summer.csv```

 * ```-l``` counts lines (i.e., records)

### *Why use it?*
 * **Instantly validate** file sizes before/after transformations
 * **Automate sanity checks** in data pipelines

---

# ğŸŒŸ Working with Multiple Files: Wildcards
Batch processing is a must in any real data project. Wildcards (```*```, ```?```, ```{}```) let me target multiple files without typos or missed cases.

### *Example:*
Preview the first 3 rows from all spring and summer CSVs:

```head -n 3 seasonal/s*.csv```

### ğŸš€ *Why Use Wildcards?*

 * **Saves time:** No need to type each file name.
 * **Prevents errors:** Avoids missing or duplicating files by mistake.
 * **Scalable:** Works for any number of files â€” perfect for automation and scripting.

<br>

ğŸ’¡ *Master wildcards to DRY out your code and automate repetitive tasks across dozens (or hundreds) of files.*

---

## ğŸ¯ Advanced Wildcards: Smart File Selection
Match exactly the files you wantâ€”nothing more, nothing less.
E.g., ```ls``` {```singh.pdf```,```j*.txt```} matches only ```singh.pdf``` and ```johel.txtâ€”not``` â€” not files like ```sandhu.txt```

### â­ï¸ Common Wildcards for File Selection

 * ```*``` (asterisk): Matches any number of characters (including none).
   Example: ```*.csv``` matches all CSV files in the directory.
 * ```?``` (question mark): Matches exactly one character.
   Example: ```201?.txt``` matches ```2017.txt```, ```2018.txt``` but not ```2017-01.txt```.
 * ```[abc]```: Matches a single character from the set.
   Example: ```201[78].txt``` matches ```2017.txt``` and ```2018.txt```.
 * ```{pattern1,pattern2}```: Matches any of the comma-separated patterns.
 Example: ```{*.txt,*.csv}``` matches all ```.txt``` and ```.csv``` files.

---

# ğŸ”€ Sorting Data in the Shell
Sorting is crucial for QA, reporting, or prepping for downstream analysis.

```cut -d , -f 2 seasonal/winter.csv | grep -v Tooth | sort -r```

* Get all tooth types in reverse (descending) order.

### *Example:*
 > The ```sort``` command has useful flags:
 * ```-n``` for numerical sort
 * ```-r``` for reverse order
 * ```-b``` to ignore leading blanks
 * ```-f``` for case-insensitive sorting

<br>

ğŸ’¡ *Combine with ```uniq``` for unique values, or add ```head/tail``` for the top/bottom -N (value).*

---

# ğŸ”„ Removing Duplicates with ```sort | uniq```
Clean, deduplicated data is vital for analysis and reporting.
Combine ```sort``` (to group) and ```uniq -c``` (to count unique):

```cut -d , -f 2 seasonal/winter.csv | grep -v Tooth | sort | uniq -c```

### *Examples:*
1. ```cut -d , -f 2 seasonal/winter.csv```
   â†’ Extracts the second column (tooth names) from the CSV.
2. ```grep -v Tooth```
   â†’ Removes the header line containing the word "Tooth".
3. ```sort```
   â†’ Sorts the tooth names alphabetically, grouping duplicates together.
4. ```uniq -c```
   â†’ Removes adjacent duplicates and prints a count of each unique tooth type.

ğŸ’¡ *```uniq``` only removes adjacent duplicate lines. That's why sorting is crucial before using ```uniq```, to make sure identical entries are grouped together.*

---

# ğŸ’¾ Saving Pipeline Output
Place your redirection (```>```) at the end of the pipelineâ€”never in the middle or front.

âœ… Correct:

```cut -d , -f 2 seasonal/*.csv | grep -v Tooth > teeth-only.txt```

âŒ Incorrect:

```cut -d , -f 2 seasonal/*.csv > teeth-only.txt | grep -v Tooth     # Won't work as expected!```

---

# â¹ï¸ Stopping Long-Running Commands
Sometimes, shell commands hang or run too long. Use ```Ctrl + C``` (```^C```) to kill the current process without closing your terminal â€” essential for troubleshooting in live data ops.

---

# ğŸš¦ Wrapping Up: Real QAâ€”Find the Shortest File
Suppose I need to quickly spot which seasonal data file has the fewest rows (for QA or merging):

```wc -l seasonal/*.csv | grep -v total | sort -n | head -n 1```

* Gets **record counts** for all files
* **Removes** the â€œtotalâ€ row
* **Sorts by** record count
* **Returns** the shortest file

### *Result:*
ğŸ’¡ *A single command for a common analyst question â€” production-ready and automation-friendly. ğŸ˜*

---

## *Bottom line:*
*The Unix shellâ€”especially when you start combining commandsâ€”lets me build robust, transparent, and repeatable data workflows that scale from ad hoc exploration to production analytics.*
