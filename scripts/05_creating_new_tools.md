# ðŸ› ï¸ Creating New Tools
In my workflow as a data analyst, reusing and automating processes is key to efficiency and accuracy. The Unix shell is not just for stringing together existing commandsâ€”it also empowers you to create your own tools and scripts, streamlining repetitive tasks and enabling advanced automation. In this chapter, I demonstrate how to write and use custom shell scripts to boost productivity in real-world data projects.

### **Key Concepts:**
* Writing and running shell scripts (```.sh``` files)
* Making scripts executable and placing them in your ```PATH```
* Using variables, arguments, and loops in scripts
* Turning complex command sequences into reusable tools
* Showcasing practical automation for data cleaning, file management, and reporting

Below, I present hands-on examples of how these scripting skills solve common data and workflow challenges.

---

# ðŸ“ Editing a File with Nano 

### ðŸŒ Real-world scenario:
Whenever I need to create or update a quick configuration file, document, or data list directly on a server or in a project, I reach for the Nano editor. Itâ€™s lightweight, always available, and easy to useâ€”perfect for batch scripting or quick fixes.

#### ðŸ’¡ Step-by-step:

**1. Open (or create) a new file for editing:**
   ```nano names.txt```

**2. Use these common Nano keyboard shortcuts:**

   * ```Ctrl + K``` : Delete a line
   * ```Ctrl + U``` : Un-delete a line
   * ```Ctrl + O``` : Save the file (press Enter to confirm the filename)
   * ```Ctrl + X``` : Exit Nano

**3. Example file content for a team list or demo data:**
```
Lovelace
Hopper
Johnson
Wilson
```

**4. Save and exit as shown above.**
This approach keeps my shell workflow fast and flexible, without ever leaving the terminal.

---

# ðŸ“ Capturing Command-Line Workflows with History
When I run complex analyses in the Unix shell, itâ€™s essential to keep a clear record of the exact commands Iâ€™ve usedâ€”both for reproducibility and knowledge sharing. Rather than manually jotting down steps in a notebook, I leverage the power of history and standard shell tools.

### ðŸŒ Real-world scenario:
Letâ€™s say I want to save the last 10 commands I executed, perhaps as documentation for a project or to troubleshoot a recent workflow. Hereâ€™s how I do it:

```history | tail -n 10 > figure-5.history```

This command grabs the last three lines from your shell history and saves them in a text file called ```steps.txt``` for easy reference or sharing.

#### Typical workflow:
* ```history``` prints the full list of previously executed commands.
* ```tail -n 10``` extracts the 10 most recent commands.
* ```> figure-5.history```redirects the output into a file for future reference.

This simple pipeline ensures I never miss a step and that my processes are fully reproducible. This approach demonstrates a key Unix philosophy: combining small, powerful tools to create efficient, auditable workflows.

### ðŸ’¡ Tip: 
> *Regularly exporting your shell history helps maintain a transparent, shareable record of your data analyses and automation tasks!*
